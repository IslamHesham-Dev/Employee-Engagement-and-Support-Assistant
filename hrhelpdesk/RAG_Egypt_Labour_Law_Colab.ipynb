{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0032f804",
   "metadata": {},
   "source": [
    "\n",
    "# 🔎 RAG on Egypt Labour Law 14/2025 — Colab Notebook\n",
    "\n",
    "This notebook builds a **Retrieval-Augmented Generation (RAG)** pipeline that:\n",
    "- Scrapes & cleans the two provided webpages (English + Arabic)\n",
    "- Splits by headers and chunks with overlap\n",
    "- Embeds with a **multilingual** Sentence-Transformers model\n",
    "- Stores vectors in **FAISS**\n",
    "- Retrieves top-*k* chunks and generates an answer using **OpenAI** (if key set) or a **local Transformers** fallback\n",
    "\n",
    "**Sources:**\n",
    "- https://eg.andersen.com/egypts-labour-law-14-2025/\n",
    "- https://manshurat.org/content/qnwn-lml-ljdyd-2025\n",
    "\n",
    "> If you have an OpenAI API key, set it in the cell below for stronger answers. Otherwise, the notebook will use a local `Flan-T5` fallback.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec671955",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@title ⬇️ Install dependencies\n",
    "!pip -q install requests beautifulsoup4 readability-lxml lxml html5lib\n",
    "!pip -q install sentence-transformers faiss-cpu\n",
    "!pip -q install openai transformers accelerate torch --upgrade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43a36b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@title 🔐 (Optional) Set your OpenAI API key\n",
    "#@markdown If you have an OpenAI key, paste it here. If not, leave blank and the notebook will use a local Transformers fallback (Flan-T5).\n",
    "OPENAI_API_KEY = \"\"  #@param {type:\"string\"}\n",
    "import os\n",
    "if OPENAI_API_KEY:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "    print(\"✅ OpenAI API key set\")\n",
    "else:\n",
    "    print(\"ℹ️ No OpenAI key provided — using local fallback model (Flan-T5).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e9c129",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@title 🧩 Imports & configuration\n",
    "import os, re, time, pickle, pathlib, logging\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from readability import Document\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Optional LLMs\n",
    "try:\n",
    "    import openai\n",
    "except Exception:\n",
    "    openai = None\n",
    "\n",
    "try:\n",
    "    from transformers import pipeline\n",
    "except Exception:\n",
    "    pipeline = None\n",
    "\n",
    "# Config\n",
    "EMBEDDING_MODEL_NAME = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"  # Arabic + English\n",
    "EMB_DIM = 384\n",
    "URLS = [\n",
    "    \"https://eg.andersen.com/egypts-labour-law-14-2025/\",\n",
    "    \"https://manshurat.org/content/qnwn-lml-ljdyd-2025\",\n",
    "]\n",
    "INDEX_DIR = pathlib.Path(\"rag_index\")\n",
    "INDEX_DIR.mkdir(exist_ok=True)\n",
    "FAISS_PATH = INDEX_DIR / \"faiss.index\"\n",
    "META_PATH = INDEX_DIR / \"metadata.pkl\"\n",
    "\n",
    "# Chunking / retrieval\n",
    "MAX_CHARS = 350\n",
    "OVERLAP = 60\n",
    "TOP_K = 6\n",
    "MAX_CONTEXT_CHARS = 8000\n",
    "\n",
    "# Generation\n",
    "OPENAI_MODEL = \"gpt-4o-mini\"\n",
    "LOCAL_LLM = \"google/flan-t5-base\"\n",
    "MAX_GEN_TOKENS = 512\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s | %(levelname)s | %(message)s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d095103f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@title 🧼 Scraping & text cleaning helpers\n",
    "def fetch_html(url: str, retries: int = 3, timeout: int = 30) -> str:\n",
    "    for i in range(retries):\n",
    "        try:\n",
    "            r = requests.get(url, timeout=timeout, headers={\"User-Agent\": \"RAG/1.0\"})\n",
    "            r.raise_for_status()\n",
    "            return r.text\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Fetch failed ({i+1}/{retries}) {url}: {e}\")\n",
    "            time.sleep(1.5)\n",
    "    raise RuntimeError(f\"Could not fetch {url}\")\n",
    "\n",
    "def readability_clean(html: str) -> str:\n",
    "    try:\n",
    "        return Document(html).summary(html_partial=True)\n",
    "    except Exception:\n",
    "        return html\n",
    "\n",
    "def html_to_text_keep_headers(html: str) -> str:\n",
    "    soup = BeautifulSoup(html, \"html5lib\")\n",
    "    for tag in soup([\"script\", \"style\", \"noscript\"]):\n",
    "        tag.decompose()\n",
    "\n",
    "    lines = []\n",
    "    ctx = soup.body if soup.body else soup\n",
    "    for el in ctx.descendants:\n",
    "        if getattr(el, \"name\", None) and re.fullmatch(r\"h[1-6]\", el.name, flags=re.I):\n",
    "            txt = el.get_text(\" \", strip=True)\n",
    "            if txt:\n",
    "                level = int(el.name[1])\n",
    "                lines.append(f\"\\n{'#'*level} {txt}\\n\")\n",
    "        elif getattr(el, \"name\", None) == \"p\":\n",
    "            txt = el.get_text(\" \", strip=True)\n",
    "            if txt:\n",
    "                lines.append(txt)\n",
    "\n",
    "    text = \"\\n\".join(lines)\n",
    "    text = re.sub(r\"[ \\t]+\", \" \", text)\n",
    "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text).strip()\n",
    "    return text\n",
    "\n",
    "def split_by_headers(text: str) -> List[Tuple[str, str]]:\n",
    "    pat = re.compile(r\"^(#{1,6})\\s+(.*)$\", flags=re.MULTILINE)\n",
    "    sections, last, title = [], 0, \"Document\"\n",
    "    for m in pat.finditer(text):\n",
    "        start = m.start()\n",
    "        if start > last:\n",
    "            body = text[last:start].strip()\n",
    "            if body:\n",
    "                sections.append((title, body))\n",
    "        title = m.group(2).strip()\n",
    "        last = m.end()\n",
    "    tail = text[last:].strip()\n",
    "    if tail:\n",
    "        sections.append((title, tail))\n",
    "    if not sections:\n",
    "        sections = [(\"Document\", text)]\n",
    "    return sections\n",
    "\n",
    "def chunk_text(text: str, max_len: int = MAX_CHARS, overlap: int = OVERLAP) -> List[str]:\n",
    "    text = text.strip()\n",
    "    if len(text) <= max_len:\n",
    "        return [text]\n",
    "    out, start = [], 0\n",
    "    while start < len(text):\n",
    "        end = min(start + max_len, len(text))\n",
    "        out.append(text[start:end].strip())\n",
    "        if end == len(text):\n",
    "            break\n",
    "        start = max(0, end - overlap)\n",
    "    return out\n",
    "\n",
    "def make_docs_from_url(url: str) -> List[Dict]:\n",
    "    html = fetch_html(url)\n",
    "    main = readability_clean(html)\n",
    "    text = html_to_text_keep_headers(main)\n",
    "    sections = split_by_headers(text)\n",
    "\n",
    "    docs, sec_id = [], 0\n",
    "    for title, body in sections:\n",
    "        body = re.sub(r\"\\n{3,}\", \"\\n\\n\", body).strip()\n",
    "        for i, chunk in enumerate(chunk_text(body)):\n",
    "            if len(chunk) < 30:\n",
    "                continue\n",
    "            docs.append({\n",
    "                \"id\": f\"{url}::sec{sec_id}::chunk{i}\",\n",
    "                \"url\": url,\n",
    "                \"section\": title,\n",
    "                \"chunk_id\": i,\n",
    "                \"text\": chunk\n",
    "            })\n",
    "        sec_id += 1\n",
    "    return docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9220646c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@title 🔢 Embeddings & FAISS index\n",
    "class Embedder:\n",
    "    def __init__(self, model_name: str = EMBEDDING_MODEL_NAME):\n",
    "        logging.info(f\"Loading embeddings model: {model_name}\")\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "\n",
    "    def encode(self, texts: List[str]) -> np.ndarray:\n",
    "        embs = self.model.encode(\n",
    "            texts,\n",
    "            batch_size=32,\n",
    "            show_progress_bar=True,\n",
    "            convert_to_numpy=True,\n",
    "            normalize_embeddings=True\n",
    "        )\n",
    "        return embs.astype(np.float32)\n",
    "\n",
    "class FaissIndex:\n",
    "    def __init__(self, dim: int, index_path: pathlib.Path, meta_path: pathlib.Path):\n",
    "        self.dim = dim\n",
    "        self.index_path = index_path\n",
    "        self.meta_path = meta_path\n",
    "               self.index = None\n",
    "        self.metadata: List[Dict] = []\n",
    "\n",
    "    def build(self, embeddings: np.ndarray, metadata: List[Dict]):\n",
    "        index = faiss.IndexFlatIP(self.dim)  # cosine if normalized\n",
    "        index.add(embeddings)\n",
    "        self.index = index\n",
    "        self.metadata = metadata\n",
    "\n",
    "    def save(self):\n",
    "        if self.index is None:\n",
    "            raise RuntimeError(\"No index to save\")\n",
    "        faiss.write_index(self.index, str(self.index_path))\n",
    "        with open(self.meta_path, \"wb\") as f:\n",
    "            pickle.dump(self.metadata, f)\n",
    "        logging.info(f\"Saved index to {self.index_path} and metadata to {self.meta_path}\")\n",
    "\n",
    "    def load(self):\n",
    "        self.index = faiss.read_index(str(self.index_path))\n",
    "        with open(self.meta_path, \"rb\") as f:\n",
    "            self.metadata = pickle.load(f)\n",
    "        logging.info(\"Loaded FAISS index & metadata\")\n",
    "\n",
    "    def search(self, query_emb: np.ndarray, top_k: int = TOP_K) -> List[Tuple[float, Dict]]:\n",
    "        if self.index is None:\n",
    "            raise RuntimeError(\"Index not loaded\")\n",
    "        D, I = self.index.search(query_emb.astype(np.float32), top_k)\n",
    "        hits = []\n",
    "        for score, idx in zip(D[0], I[0]):\n",
    "            if idx == -1:\n",
    "                continue\n",
    "            hits.append((float(score), self.metadata[idx]))\n",
    "        return hits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bdc93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@title 🧠 Answer generation (OpenAI or local Transformers)\n",
    "class AnswerGenerator:\n",
    "    def __init__(self):\n",
    "        self.use_openai = bool(openai) and bool(os.getenv(\"OPENAI_API_KEY\"))\n",
    "        if self.use_openai:\n",
    "            openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "            logging.info(\"Using OpenAI for generation.\")\n",
    "        else:\n",
    "            if not pipeline:\n",
    "                raise RuntimeError(\"Transformers not installed and no OPENAI_API_KEY set.\")\n",
    "            logging.info(\"Using local transformers (Flan-T5) for generation.\")\n",
    "            self.pipe = pipeline(\"text2text-generation\", model=LOCAL_LLM)\n",
    "\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        if self.use_openai:\n",
    "            resp = openai.chat.completions.create(\n",
    "                model=OPENAI_MODEL,\n",
    "                temperature=0.2,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                max_tokens=MAX_GEN_TOKENS,\n",
    "            )\n",
    "            return resp.choices[0].message.content.strip()\n",
    "        else:\n",
    "            out = self.pipe(prompt, max_new_tokens=MAX_GEN_TOKENS, do_sample=False)[0][\"generated_text\"]\n",
    "            return out.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5fd780",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@title 🧱 RAG pipeline\n",
    "class RAGPipeline:\n",
    "    def __init__(self, urls: List[str], embedder: Embedder, index: FaissIndex, generator: AnswerGenerator):\n",
    "        self.urls = urls\n",
    "        self.embedder = embedder\n",
    "        self.index = index\n",
    "        self.generator = generator\n",
    "\n",
    "    def ingest(self, force_rebuild: bool = False):\n",
    "        if FAISS_PATH.exists() and META_PATH.exists() and not force_rebuild:\n",
    "            logging.info(\"Index exists—loading from disk.\")\n",
    "            self.index.load()\n",
    "            return\n",
    "\n",
    "        all_docs: List[Dict] = []\n",
    "        for url in self.urls:\n",
    "            docs = make_docs_from_url(url)\n",
    "            logging.info(f\"{url} → {len(docs)} chunks\")\n",
    "            all_docs.extend(docs)\n",
    "\n",
    "        embeddings = self.embedder.encode([d[\"text\"] for d in all_docs])\n",
    "        self.index.build(embeddings, all_docs)\n",
    "        self.index.save()\n",
    "\n",
    "    def _build_prompt(self, query: str, retrieved: List[Tuple[float, Dict]]) -> str:\n",
    "        parts, total = [], 0\n",
    "        for score, m in retrieved:\n",
    "            block = f\"\\n[Source: {m['url']} | Section: {m.get('section','')}] Score={score:.3f}\\n{m['text']}\\n\"\n",
    "            if total + len(block) > MAX_CONTEXT_CHARS:\n",
    "                break\n",
    "            parts.append(block)\n",
    "            total += len(block)\n",
    "\n",
    "        context = \"\\n\".join(parts).strip()\n",
    "        instructions = (\n",
    "            \"You are a legal assistant answering about Egypt’s Labour Law 14/2025.\\n\"\n",
    "            \"Ground answers ONLY in the context below (Arabic or English). If unsure, say so.\\n\"\n",
    "            \"Where relevant, cite the URL in-line as (Source: <url>). Use bullets when helpful.\"\n",
    "        )\n",
    "\n",
    "        return f\"\"\"{instructions}\n",
    "\n",
    "Question:\n",
    "{query}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Answer (concise and specific):\n",
    "\"\"\"\n",
    "\n",
    "    def retrieve(self, query: str) -> List[Tuple[float, Dict]]:\n",
    "        q_emb = self.embedder.encode([query])\n",
    "        return self.index.search(q_emb, top_k=TOP_K)\n",
    "\n",
    "    def answer(self, query: str) -> Dict:\n",
    "        hits = self.retrieve(query)\n",
    "        prompt = self._build_prompt(query, hits)\n",
    "        text = self.generator.generate(prompt)\n",
    "        return {\"query\": query, \"answer\": text, \"retrieved\": hits}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f658d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@title 🚀 Build the index and run demo questions\n",
    "embedder = Embedder()\n",
    "index = FaissIndex(EMB_DIM, FAISS_PATH, META_PATH)\n",
    "generator = AnswerGenerator()\n",
    "rag = RAGPipeline(URLS, embedder, index, generator)\n",
    "\n",
    "# Set force_rebuild=True to re-scrape/re-embed\n",
    "rag.ingest(force_rebuild=False)\n",
    "\n",
    "questions = [\n",
    "    \"What are the key changes in Egypt’s Labour Law 14/2025?\",\n",
    "    \"What are the rules for maternity leave in Egypt?\"\n",
    "]\n",
    "\n",
    "def pretty(result: Dict):\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Q:\", result[\"query\"])\n",
    "    print(\"-\" * 80)\n",
    "    print(result[\"answer\"])\n",
    "    print(\"-\" * 80)\n",
    "    print(\"Top sources:\")\n",
    "    seen = set()\n",
    "    for score, m in result[\"retrieved\"]:\n",
    "        if m[\"id\"] in seen:\n",
    "            continue\n",
    "        seen.add(m[\"id\"])\n",
    "        print(f\"* {m['url']} | Section: {m.get('section','')} | Score: {score:.3f}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "for q in questions:\n",
    "    pretty(rag.answer(q))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7bea77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@title 💬 Ask your own questions\n",
    "your_question = \"Summarize penalties related to working hours\"  #@param {type:\"string\"}\n",
    "res = rag.answer(your_question)\n",
    "print(res[\"answer\"])\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
